import requests, json
from bs4 import BeautifulSoup

response = requests.get(f'https://justbible.ru/api/search?translation=rst&search=ÐÐ°Ñ‡Ð°Ð»Ð¾')
data = response.json()
l = len(data) - 1
print(f'ÐÐ°ÑˆÐ»Ð¾ {l} Ð¡Ð¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹')
for i in range(l):
    count = data[f'{i}']
    d = count['data']
    text = count['text']
    print(d)
    print(text)
    print()
    

# with open('bot/cache/bible.json', 'r', encoding='utf-8') as file:
#     data = json.load(file)

# data = data['ÐœÐ°Ñ‚Ñ„ÐµÐ¹']
# data = data['1']
# for i in range(1, len(data) + 1):
#     print(i, data[f'{i}'])
#     print()
# # ÐŸÐµÑ€ÐµÐ±Ð¾Ñ€ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ð¹ ÐºÐ½Ð¸Ð³ (Ð³Ð»Ð°Ð²)
# print("ðŸ“š Ð’ÑÐµ ÐºÐ½Ð¸Ð³Ð¸ Ð‘Ð¸Ð±Ð»Ð¸Ð¸:")
# for book_name in data.keys():
#     print(f"â€¢ {book_name}")

# # Ð˜Ð»Ð¸ Ñ Ð½ÑƒÐ¼ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹
# print("\nðŸ“š Ð’ÑÐµ ÐºÐ½Ð¸Ð³Ð¸ Ð‘Ð¸Ð±Ð»Ð¸Ð¸ (Ñ Ð½ÑƒÐ¼ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹):")
# for i, book_name in enumerate(data.keys(), 1):
#     print(f"{i}. {book_name}")
    

# import requests
# import json
# from datetime import datetime

# def get_daily_verse():
#     """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½ÑƒÑŽ Ð±Ð¸Ð±Ð»ÐµÐ¹ÑÐºÑƒÑŽ Ñ†Ð¸Ñ‚Ð°Ñ‚Ñƒ"""
#     try:
#         response = requests.get('https://bible-api.com/?random=verse')
#         data = response.json()
#         return {
#             'text': data['text'],
#             'reference': data['reference'],
#             'version': data['translation_id']
#         }
#     except Exception as e:
#         print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ†Ð¸Ñ‚Ð°Ñ‚Ñ‹: {e}")
#         return None

# def print_verse(verse):
#     """ÐšÑ€Ð°ÑÐ¸Ð²Ð¾ Ð²Ñ‹Ð²ÐµÑÑ‚Ð¸ Ñ†Ð¸Ñ‚Ð°Ñ‚Ñƒ"""
#     if verse:
#         print(f"\nðŸ“– Ð‘Ð¸Ð±Ð»ÐµÐ¹ÑÐºÐ°Ñ Ñ†Ð¸Ñ‚Ð°Ñ‚Ð° Ð½Ð° {datetime.now().strftime('%d.%m.%Y')}")
#         print("=" * 50)
#         print(f"{verse['text']}")
#         print(f"\nâ€” {verse['reference']} ({verse['version']})")
#         print("=" * 50)

# # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ
# if __name__ == "__main__":
#     verse = get_daily_verse()
#     print_verse(verse)

# import requests
# from bs4 import BeautifulSoup

# # 2. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
# url = 'https://dobslovo.ru/arhivy-gazety/'
# response = requests.get(url)

# # 2. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
# if response.status_code == 200:
#     soup = BeautifulSoup(response.text, 'html.parser')
    
#     test = soup.find_all('div', class_='elementor-icon-wrapper')
#     for blog in test:
#         link = blog.find('a', class_='elementor-icon')
#         if link:
#             href = link.get('href')
#             if href:
#                 # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ https ÐµÑÐ»Ð¸ ÑÑÑ‹Ð»ÐºÐ° Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ñ //
#                 if href.startswith('//'):
#                     href = 'https:' + href
#                 # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ PDF Ñ„Ð°Ð¹Ð»
#                 if href.endswith('pdf'):
#                     filepapers = href.split('/')[-1].replace('.pdf', '')
#                     num_papers = filepapers.split('-')
#                     # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ Ð¼Ð°ÑÑÐ¸Ð² Ð¸Ð¼ÐµÐµÑ‚ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
#                     if len(num_papers) >= 3:
#                         years = num_papers[1]
#                         count = num_papers[-2]
#                         text = f"{href}\n{filepapers}\n{years}-{count}\n-------------"
#                         print(text)
#         img_tag = blog.find_previous("img")
#         img_url = img_tag["src"] if img_tag else None
#         print('image: ' + img_url)
        
    
    # test = soup.find('span', class_='elementor-counter-number')
    # test = test.get('data-to-value')
    # print(test)
    
    # blog_items = soup.find_all('div', class_='elementor-toggle-item')
    # # print(blog_items)
    # for blog in blog_items:
    #     name = blog.find('a', class_='elementor-toggle-title').text.strip()
    #     text = blog.find('div', class_='elementor-tab-content').text.strip()
    #     print(name, '----', text)